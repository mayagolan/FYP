{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx6rEKJXktVO"
      },
      "source": [
        "# Install Relevant Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhGgVpRmkgHw"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!pip install BeautifulSoup4\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OevF1kB6kxKt"
      },
      "source": [
        "# Import Relevant Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoL_nRBJkydg"
      },
      "outputs": [],
      "source": [
        "import  numpy as np \n",
        "import  matplotlib.pyplot as plt \n",
        "import  pandas as pd\n",
        "import  os\n",
        "import  re \n",
        "from    selenium import webdriver\n",
        "from    selenium.webdriver.common.keys import Keys\n",
        "from    selenium.webdriver.support import expected_conditions as EC\n",
        "from    selenium.webdriver.support.ui import WebDriverWait\n",
        "from    selenium.webdriver.common.by import By\n",
        "from    selenium.webdriver.common.action_chains import ActionChains\n",
        "import  time\n",
        "import  urllib3\n",
        "from    bs4 import BeautifulSoup\n",
        "import  csv\n",
        "import  requests \n",
        "from    urllib import request\n",
        "from    google.colab import files\n",
        "import  sys\n",
        "from    selenium import webdriver\n",
        "import  json \n",
        "from    urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oU4H6XNYZW5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "df=pd.read_csv('gdrive/My Drive/Colab Notebooks/FYP/Reuters List.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUCLUTK5wT10"
      },
      "source": [
        "# Import the File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-ta3Lirk1Ux"
      },
      "source": [
        "# Web Scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1vpoQAOk1CJ"
      },
      "outputs": [],
      "source": [
        "for index, row in df.iloc[0:1].iterrows():\n",
        "    sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "    company_name = row['description']\n",
        "    print(company_name)\n",
        "    url = row['url']\n",
        "    \n",
        "    if not pd.isna(url):\n",
        "      print(url)\n",
        "      driver.get(url)\n",
        "      #wait = WebDriverWait(driver, 40)\n",
        "      #button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"accept-recommended-btn-handler\"]')))\n",
        "      #driver.execute_script(\"arguments[0].click()\", button)\n",
        "      \n",
        "      scrollNumber = 100\n",
        "\n",
        "      for i in range(0, 100):\n",
        "      \n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        # time.sleep(3)\n",
        "      \n",
        "      html = driver.page_source\n",
        "      f = open('{0}.html'.format(company_name), 'w')\n",
        "      f.write(html.encode('utf-8').decode('ascii', 'ignore'))\n",
        "      f.close()\n",
        "      driver.close()\n",
        "      http=urllib3.PoolManager()\n",
        "      data = open('{0}.html'.format(company_name),'r')\n",
        "      \n",
        "      soup = BeautifulSoup(data, 'html.parser')\n",
        "      data1 = soup.find('div', attrs = {'class': 'FeedScroll-feed-container-106s7'})\n",
        "      \n",
        "      links = []\n",
        "      for link in data1.find_all('a'): \n",
        "        links.append(link.get('href'))\n",
        "      \n",
        "      links = list(dict.fromkeys(links))\n",
        "\n",
        "      locals()[\"df\" + str(company_name)] = pd.DataFrame(columns = ['Date', 'Headline', 'Text'])\n",
        "\n",
        "      for link in links:\n",
        "        try:\n",
        "          url = link\n",
        "          page = urlopen(url)\n",
        "          html_bytes = page.read()\n",
        "          html = html_bytes.decode(\"utf-8\")\n",
        "          soup = BeautifulSoup(html, 'html.parser')\n",
        "          try:\n",
        "              head = soup.find('h1', attrs = {'class': 'Text__text___3eVx1j Text__dark-grey___AS2I_p Text__medium___1ocDap Text__heading_2___sUlNJP Heading__base___1dDlXY Heading__heading_2___3f_bIW'})\n",
        "              headline = head.string\n",
        "              date = soup.find('span', attrs = {'class': 'DateLine__date___12trWy'})\n",
        "              final_date = date.string\n",
        "              text = soup.find('div', attrs = {'class': 'ArticleBody__content___2gQno2 paywall-article'})\n",
        "              paragraphs = text.find_all('p')\n",
        "              text_final = \"\"\n",
        "              for paragraph in paragraphs:\n",
        "                  text_final = text_final + paragraph.getText()\n",
        "          except:\n",
        "              try:\n",
        "                  head = soup.find('h1', attrs = {'class': 'Headline-headline-2FXIq Headline-black-OogpV ArticleHeader-headline-NlAqj'})\n",
        "                  headline = head.string\n",
        "                  date = json.loads(soup.select_one('[type=\"application/ld+json\"]').contents[0])\n",
        "                  final_date = date[\"datePublished\"]\n",
        "                  text = soup.find_all('p', attrs = {'class': 'Paragraph-paragraph-2Bgue ArticleBody-para-TD_9x'})\n",
        "                  text_final = \"\"\n",
        "                  for paragraph in text:\n",
        "                    text_final = text_final + paragraph.getText()\n",
        "              except:\n",
        "                  pass \n",
        "        except:\n",
        "          pass\n",
        "      \n",
        "        locals()[\"df2\" + str(company_name)] = pd.DataFrame({\"Date\":[final_date], \"Headline\":[headline], \"Text\":[text_final]})\n",
        "        locals()[\"df\" + str(company_name)] = locals()[\"df\" + str(company_name)].append(locals()[\"df2\" + str(company_name)])\n",
        "      \n",
        "      locals()[\"df\" + str(company_name)].to_csv('{0}.csv'.format(company_name))\n",
        "      # [\"df\" + str(company_name)].to_csv('{0}.csv'.format(company_name))\n",
        "      # files.download('{0}.csv'.format(company_name))\n",
        "      text = '{0}.csv'.format(company_name)\n",
        "      print(text)\n",
        "      !cp '{0}.csv'.format(company_name) \"gdrive/My Drive/Colab Notebooks/FYP/REUTERS\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Web Scraper - Reuters.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}