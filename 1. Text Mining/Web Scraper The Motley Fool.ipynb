{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeJsk2seVXbx"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9HVkUcXVoxD"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import os\n",
        "import re \n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "import urllib3\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "# import requests \n",
        "from urllib import request\n",
        "from google.colab import files\n",
        "import sys\n",
        "from selenium import webdriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFbLM9NqVsTI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "df=pd.read_csv('gdrive/My Drive/Colab Notebooks/FYP/TMF List.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8za9ajWnVwaj"
      },
      "outputs": [],
      "source": [
        "for index, row in df.iloc[458:460].iterrows():\n",
        "    sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "    company_name = row['description']\n",
        "    print(company_name)\n",
        "    url = row['url']\n",
        "    print(url)\n",
        "\n",
        "    if not pd.isna(url):\n",
        "      driver.get(url)\n",
        "      wait = WebDriverWait(driver, 20)\n",
        "      # XPATHS COOKIE MARKETMOVES = [ ' //*[@id=\"modal−form\"]/fieldset/ul[1]/li[1]/label/input ' , ' //*[@id=\"modal−form\"]/fieldset/ul[2]/li[1]/label/input ' , ' //*[@id=\"gdpr−submit−button\"] ' ]\n",
        "\n",
        "      # for i in range(0, 3):\n",
        "      #   button = wait.until(EC.element to be clickable ((By.XPATH , XPATHS COOKIE MARKETMOVES[i])))\n",
        "      #   driver.execute script(\"arguments[0].click()\", button)\n",
        "      #   time.sleep(3)\n",
        "      \n",
        "      try:\n",
        "        while(driver.find_element_by_xpath('//*[@id=\"load-more\"]').is_displayed()):\n",
        "          element = driver.find_element_by_xpath('//*[@id=\"load-more\"]')\n",
        "          driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
        "          time.sleep(3)\n",
        "          button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"load-more\"]')))\n",
        "          driver.execute_script(\"arguments[0].click()\", button)\n",
        "          time.sleep(3)\n",
        "\n",
        "      except:\n",
        "        pass \n",
        "\n",
        "      html = driver.page_source\n",
        "      f = open('{0}.html'.format(company_name), 'w')\n",
        "      f.write(html.encode('utf-8').decode('ascii', 'ignore'))\n",
        "      f.close()\n",
        "      driver.close()\n",
        "      http=urllib3.PoolManager()\n",
        "      data = open('{0}.html'.format(company_name),'r')\n",
        "      \n",
        "      soup = BeautifulSoup(data, 'html.parser')\n",
        "      data1 = soup.find('div', attrs = {'class': 'quote-page-article-listing'})\n",
        "      links = []\n",
        "      for link in data1.find_all('a'): \n",
        "        links.append(link.get('href'))\n",
        "      \n",
        "      links = list(dict.fromkeys(links))\n",
        "      newlinks = [x for x in links if not x.startswith('https')]\n",
        "      newlinks = [x for x in newlinks if not x.startswith('/earnings')]\n",
        "\n",
        "      locals()[\"df\"+str(company_name)] = pd.DataFrame(columns = ['Author', 'Date', 'Headline', 'Text'])\n",
        "\n",
        "\n",
        "      for link in newlinks: \n",
        "        try:\n",
        "          url = \"https://www.fool.com\" + link \n",
        "          html = request.urlopen(url).read().decode('utf8')\n",
        "          soup = BeautifulSoup(html, 'html.parser')\n",
        "          head = soup.find('section', attrs = {'class': 'usmf-new article-header'})\n",
        "          headline = head.find('h1').string\n",
        "          sub_headline = head.find('h2').string\n",
        "          author = soup.find('div', attrs = {'class': 'author-name'}).find('a').string\n",
        "          dates = soup.find_all('div', attrs = {'class': 'publication-date'})\n",
        "          dates_list = []\n",
        "          for date in dates: \n",
        "              dates_list.append(date.contents[2].strip())\n",
        "\n",
        "          for i in dates_list: \n",
        "              if i.startswith('Published') and len(dates_list) > 1: \n",
        "                  date_final = i\n",
        "              else: \n",
        "                  date_final = i \n",
        "\n",
        "          text = soup.find('span', attrs = {'class': 'article-content'})\n",
        "          paragraphs = text.find_all('p')\n",
        "          text_final = \"\"\n",
        "          for paragraph in paragraphs:\n",
        "              text_final = text_final + paragraph.getText()\n",
        "\n",
        "          text_final = text_final.replace('\\\\', '')\n",
        "          locals()[\"df2\"+str(company_name)] = pd.DataFrame({\"Author\":[author], \"Date\":[date_final], \"Headline\":[headline], \"Text\":[text_final]})\n",
        "          locals()[\"df\"+str(company_name)] = locals()[\"df\"+str(company_name)].append(locals()[\"df2\"+str(company_name)])\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "      locals()[\"df\"+str(company_name)].to_csv(r'{0}.csv'.format(company_name))\n",
        "      files.download(\"{0}.csv\".format(company_name))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Web Scraper - TMF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}